train:
  max_steps: 19073 # 19,073 steps is ~1 epoch, if data is 10B tokens and batch size 0.5M tokens
  manual_seed: 1337
  batch_size: 4
  sequence_length: 1024
  mixed_precision_training:
    enable: True
    dtype: BF16 # Allowed Values: FP32, FP16, BF16, TF32

optimizer:
  name: AdamSGD
  params:
    warmup_steps: 715
    lr: 0.001
    weight_decay: 0.1
    max_lr: 0.0006 # 6e-4 From GPT3 paper
    min_lr: 0.00006 # 6e-5 From GPT3 paper
    betas: [0.9, 0.95]
